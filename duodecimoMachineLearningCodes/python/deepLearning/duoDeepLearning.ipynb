{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Deep Learning Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import scipy\n",
    "from scipy.special import expit\n",
    "from PIL import Image\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    :param x: numpy array of shape (N,D) representing input layer\n",
    "    :return:\n",
    "    out = numpy array of shape (N,D) representing output of sigmoid layer\n",
    "    cache = storing x for backpropagation\n",
    "    \"\"\"\n",
    "    cache = x.copy()\n",
    "    out = 1./(1. + np.exp(-x))\n",
    "    #out = 1./(1. + expit(-x))\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:  [[ 0.73105858  0.26894142  0.5       ]\n",
      " [ 0.88079708  0.5         0.11920292]]\n",
      "cache:  [[ 1. -1.  0.]\n",
      " [ 2.  0. -2.]]\n"
     ]
    }
   ],
   "source": [
    "# test sigmoid\n",
    "\n",
    "x=np.array([[1.0, -1.0, 0.0],[2.0, 0.0, -2.0]])\n",
    "out, cache = sigmoid(x)\n",
    "print(\"out: \", out)\n",
    "print(\"cache: \", cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dSigmoid(dout, cache):\n",
    "    \"\"\"\n",
    "    :param dout: numpy array of shape (N,D) representing gradients of output layer\n",
    "    :param cache: numpy array of shape (N,D) used for backpropagation\n",
    "    :return:\n",
    "    dx = numpy array of shape (N,D) representing gradients of input layer\n",
    "    \"\"\"\n",
    "    x = cache\n",
    "    out = 1./(1 + np.exp(-x))\n",
    "    dx = out*(1-out)\n",
    "    return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx:  [[ 0.19661193  0.19661193  0.25      ]\n",
      " [ 0.10499359  0.25        0.10499359]]\n"
     ]
    }
   ],
   "source": [
    "# test dSigmoid\n",
    "x=np.array([[ 0.73105858, 0.26894142, 0.5],[ 0.88079708, 0.5, 0.11920292]])\n",
    "cache= np.array([[ 1., -1., 0.],[ 2., 0., -2.]])\n",
    "dx = dSigmoid(x, cache)\n",
    "print(\"dx: \", dx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    :param x: numpy array of shape (N,D) representing input layer\n",
    "    :return:\n",
    "    out = numpy array of shape (N,D) representing output of tanh layer\n",
    "    cache = storing x for backpropagation\n",
    "    \"\"\"\n",
    "    cache = x.copy()\n",
    "    out = np.tanh(x)\n",
    "    return out, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:  [[ 0.76159416 -0.76159416  0.        ]\n",
      " [ 0.96402758  0.         -0.96402758]]\n",
      "cache:  [[ 1. -1.  0.]\n",
      " [ 2.  0. -2.]]\n"
     ]
    }
   ],
   "source": [
    "# test tanh\n",
    "x=np.array([[1.0, -1.0, 0.0],[2.0, 0.0, -2.0]])\n",
    "out, cache = tanh(x)\n",
    "print(\"out: \", out)\n",
    "print(\"cache: \", cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dTanh(dout, cache):\n",
    "    \"\"\"\n",
    "    :param dout: numpy array of shape (N,D) representing gradients of output layer\n",
    "    :param cache: numpy array of shape (N,D) representing input layer for backpropagation\n",
    "    :return:\n",
    "    dx = numpy array of shape (N,D) representing gradients of input layer\n",
    "    \"\"\"\n",
    "    x = cache\n",
    "    out = np.tanh(x)\n",
    "    dx = dout*(1-out**2)\n",
    "    return dx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx:  [[ 0.30702585  0.1129485   0.5       ]\n",
      " [ 0.06222904  0.5         0.00842178]]\n"
     ]
    }
   ],
   "source": [
    "# test dTanh\n",
    "x=np.array([[ 0.73105858, 0.26894142, 0.5],[ 0.88079708, 0.5, 0.11920292]])\n",
    "cache= np.array([[ 1., -1., 0.],[ 2., 0., -2.]])\n",
    "dx = dTanh(x, cache)\n",
    "print(\"dx: \", dx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    :param x: numpy array of shape (N,D) representing input layer\n",
    "    :return:\n",
    "    out = numpy array of shape (N,D) representing output of relu layer\n",
    "    cache = storing x for backpropagation\n",
    "    \"\"\"\n",
    "    cache = x.copy()\n",
    "    out = x*(x > 0)\n",
    "    #out = np.maximum(0.01, x)\n",
    "    return out, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:  [[ 1. -0.  0.]\n",
      " [ 2.  0. -0.]]\n",
      "cache:  [[ 1. -1.  0.]\n",
      " [ 2.  0. -2.]]\n"
     ]
    }
   ],
   "source": [
    "# test relu\n",
    "x=np.array([[1.0, -1.0, 0.0],[2.0, 0.0, -2.0]])\n",
    "out, cache = relu(x)\n",
    "print(\"out: \", out)\n",
    "print(\"cache: \", cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dRelu(dout, cache):\n",
    "    \"\"\"\n",
    "    :param dout: numpy array of shape (N,D) representing gradients of output layer\n",
    "    :param cache: numpy array of shape (N,D) representing input layer for backpropagation\n",
    "    :return:\n",
    "    dx = numpy array of shape (N,D) representing gradients of input layer\n",
    "    \"\"\"\n",
    "    x = cache\n",
    "    #dx = dout*(x > 0)\n",
    "    dx = 1. * (x > 0)\n",
    "    return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx:  [[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# test dRelu\n",
    "x=np.array([[ 0.73105858, 0.26894142, 0.5],[ 0.88079708, 0.5, 0.11920292]])\n",
    "cache= np.array([[ 1., -1., 0.],[ 2., 0., -2.]])\n",
    "dx = dRelu(x, cache)\n",
    "print(\"dx: \", dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "  \"\"\"\n",
    "  Arguments:\n",
    "  layer_dims -- python array (list) containing the dimensions of each layer in\n",
    "    our network\n",
    "  Returns:\n",
    "  parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "  Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "  bl -- bias vector of shape (layer_dims[l], 1)\n",
    "  \"\"\"\n",
    "\n",
    "  np.random.seed(3)\n",
    "  parameters = {}\n",
    "  L = len(layer_dims)\n",
    "  # number of layers in the network\n",
    "  for l in range(1, L):\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "    parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    ### END CODE HERE ###\n",
    "    assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "    assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "  return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "  \"\"\"\n",
    "  Implement the linear part of a layer's forward propagation.\n",
    "  Arguments:\n",
    "  A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "  W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "  b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "  Returns:\n",
    "  Z -- the input of the activation function, also called pre-activation parameter\n",
    "  cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "  \"\"\"\n",
    "\n",
    "  ### START CODE HERE ### (≈ 1 line of code)\n",
    "  Z = W.dot(A) + b\n",
    "  ### END CODE HERE ###\n",
    "  assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "  cache = (A, W, b)\n",
    "  return Z, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "  \"\"\"\n",
    "  Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "  Arguments:\n",
    "  A_prev -- activations from previous layer (or input data): (size of previouslayer, number of examples)\n",
    "  W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "  b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "  activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "  Returns:\n",
    "  A -- the output of the activation function, also called the post-activation value\n",
    "  cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "  stored for computing the backward pass efficiently\n",
    "  \"\"\"\n",
    "  Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "  if activation == \"sigmoid\":\n",
    "    # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "    A, activation_cache = sigmoid(Z)\n",
    "  elif activation == \"relu\":\n",
    "    # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "    A, activation_cache = relu(Z)\n",
    "  assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "  cache = (linear_cache, activation_cache)\n",
    "  return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "  \"\"\"\n",
    "  Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "  Arguments:\n",
    "  X -- data, numpy array of shape (input size, number of examples)\n",
    "  parameters -- output of initialize_parameters_deep()\n",
    "  Returns:\n",
    "  AL -- last post-activation value\n",
    "  caches -- list of caches containing:\n",
    "  every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "  the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "  \"\"\"\n",
    "  caches = []\n",
    "  A = X\n",
    "  L = len(parameters) // 2 # number of layers in the neural network\n",
    "  # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "  for l in range(1, L):\n",
    "    # ??? A_prev = A\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    W = parameters['W' + str(l)]\n",
    "    b = parameters['b' + str(l)]\n",
    "    A, cache = linear_activation_forward(A, W, b, activation = \"relu\")\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "  # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "  ### START CODE HERE ### (≈ 2 lines of code)\n",
    "  W = parameters['W' + str(L)]\n",
    "  b = parameters['b' + str(L)]\n",
    "  AL, cache = linear_activation_forward(A, W, b, activation = \"sigmoid\")\n",
    "\n",
    "  caches.append(cache)\n",
    "  ### END CODE HERE ###\n",
    "  assert(AL.shape == (1,X.shape[1]))\n",
    "  return AL, caches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "  \"\"\"\n",
    "  Implement the cost function defined by equation (7).\n",
    "  Arguments:\n",
    "  AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "  Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "  Returns:\n",
    "  cost -- cross-entropy cost\n",
    "  \"\"\"\n",
    "  m = Y.shape[1]\n",
    "  # Compute loss from aL and y.\n",
    "  ### START CODE HERE ### (≈ 1 lines of code)\n",
    "  cost = - np.sum((Y.dot(np.log(AL.T))+((1-Y).dot(np.log(1-AL.T)))))/m\n",
    "  ### END CODE HERE ###\n",
    "  cost = np.squeeze(cost)\n",
    "  # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "  assert(cost.shape == ())\n",
    "  return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "  \"\"\"\n",
    "  Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "  Arguments:\n",
    "  dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "  cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "  Returns:\n",
    "  dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "  dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "  db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "  \"\"\"\n",
    "  A_prev, W, b = cache\n",
    "  m = A_prev.shape[1]\n",
    "  ### START CODE HERE ### (≈ 3 lines of code)\n",
    "  dW = dZ.dot(A_prev.T)/m\n",
    "  db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "  dA_prev = W.T.dot(dZ)\n",
    "  ### END CODE HERE ###\n",
    "  assert (dA_prev.shape == A_prev.shape)\n",
    "  assert (dW.shape == W.shape)\n",
    "  assert (db.shape == b.shape)\n",
    "  return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "  \"\"\"\n",
    "  Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "  Arguments:\n",
    "  dA -- post-activation gradient for current layer lcache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "  activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "  Returns:\n",
    "  dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "  dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "  db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "  \"\"\"\n",
    "\n",
    "  linear_cache, activation_cache = cache\n",
    "  if activation == \"relu\":\n",
    "    dZ = dRelu(dA, activation_cache)\n",
    "  elif activation == \"sigmoid\":\n",
    "    dZ = dSigmoid(dA, activation_cache)\n",
    "  dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "  return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "  \"\"\"\n",
    "  Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "  Arguments:\n",
    "  AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "  Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "  caches -- list of caches containing:\n",
    "    every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "    the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "  Returns:\n",
    "  grads -- A dictionary with the gradients\n",
    "    grads[\"dA\" + str(l)] = ...\n",
    "    grads[\"dW\" + str(l)] = ...\n",
    "    grads[\"db\" + str(l)] = ...\n",
    "  \"\"\"\n",
    "  grads = {}\n",
    "  L = len(caches) # the number of layers\n",
    "  m = AL.shape[1]\n",
    "  Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "  # Initializing the backpropagation\n",
    "  ### START CODE HERE ### (1 line of code)\n",
    "  dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "  ### END CODE HERE ###\n",
    "  # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "  ### START CODE HERE ### (approx. 2 lines)\n",
    "  current_cache = caches[L-1]\n",
    "  grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "  ### END CODE HERE ###\n",
    "  for l in reversed(range(L-1)):\n",
    "    # lth layer: (RELU -> LINEAR) gradients.\n",
    "    # Inputs: \"grads[\"dA\" + str(l + 2)], caches\".\n",
    "    # Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)]\n",
    "    ### START CODE HERE ### (approx. 5 lines)\n",
    "    current_cache = caches[l]\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, \"relu\")\n",
    "    grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "    grads[\"db\" + str(l + 1)] = db_temp\n",
    "    ### END CODE HERE ###\n",
    "  return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "  \"\"\"\n",
    "  Update parameters using gradient descent\n",
    "  Arguments:\n",
    "  parameters -- python dictionary containing your parameters\n",
    "  grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "  Returns:\n",
    "  parameters -- python dictionary containing your updated parameters\n",
    "    parameters[\"W\" + str(l)] = ...\n",
    "    parameters[\"b\" + str(l)] = ...\n",
    "  \"\"\"\n",
    "\n",
    "  L = len(parameters) // 2 # number of layers in the neural network\n",
    "  # Update rule for each parameter. Use a for loop.\n",
    "  ### START CODE HERE ### (≈ 3 lines of code)\n",
    "  for l in range(L):\n",
    "    parameters[\"W\" + str(l+1)] -= learning_rate * grads[\"dW\" + str(l+1)]\n",
    "    parameters[\"b\" + str(l+1)] -= learning_rate * grads[\"db\" + str(l+1)]\n",
    "    ### END CODE HERE ###\n",
    "  return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "  \"\"\"\n",
    "  Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "  Arguments:\n",
    "  X -- input data, of shape (n_x, number of examples)\n",
    "  Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "  layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "  num_iterations -- number of iterations of the optimization loop\n",
    "  learning_rate -- learning rate of the gradient descent update rule\n",
    "  print_cost -- If set to True, this will print the cost every 100 iterations\n",
    "  Returns:\n",
    "  parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "  \"\"\"\n",
    "  np.random.seed(1)\n",
    "  grads = {}\n",
    "  costs = []\n",
    "  # to keep track of the cost\n",
    "  m = X.shape[1]\n",
    "  (n_x, n_h, n_y) = layers_dims\n",
    "  # number of examples\n",
    "  # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "  ### START CODE HERE ### (≈ 1 line of code)\n",
    "  parameters = initialize_parameters(layers_dims)\n",
    "  ### END CODE HERE ###\n",
    "  # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "  W1 = parameters[\"W1\"]\n",
    "  b1 = parameters[\"b1\"]\n",
    "  W2 = parameters[\"W2\"]\n",
    "  b2 = parameters[\"b2\"]\n",
    "  # Loop (gradient descent)\n",
    "  for i in range(0, num_iterations):\n",
    "    # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1\". Output: \"A1, cache1, A2, cache2\".\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A1, cache1 = linear_activation_forward(X, W1, b1, \"relu\")\n",
    "    A2, cache2 = linear_activation_forward(A1, W2, b2, \"sigmoid\")\n",
    "    ### END CODE HERE ###\n",
    "    # Compute cost\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    cost = compute_cost(A2, Y)\n",
    "    ### END CODE HERE ###\n",
    "    # Initializing backward propagation\n",
    "    dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "    # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    dA1, dW2, db2 = linear_activation_backward(dA2, cache2, \"sigmoid\")\n",
    "    dA0, dW1, db1 = linear_activation_backward(dA1, cache1, \"relu\")\n",
    "    ### END CODE HERE ###\n",
    "    # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "    grads['dW1'] = dW1\n",
    "    grads['db1'] = db1\n",
    "    grads['dW2'] = dW2\n",
    "    grads['db2'] = db2\n",
    "    # Update parameters.\n",
    "    ### START CODE HERE ### (approx. 1 line of code)\n",
    "    parameters = update_parameters(parameters, grads, learning_rate)\n",
    "    ### END CODE HERE ###\n",
    "    # Retrieve W1, b1, W2, b2 from parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    # Print the cost every 100 training example\n",
    "    if print_cost and i % 100 == 0:\n",
    "      print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "    if print_cost and i % 100 == 0:\n",
    "      costs.append(cost)\n",
    "  return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#\n",
    "# Functions for downloading and extracting data-files from the internet.\n",
    "#\n",
    "# Implemented in Python 3.5\n",
    "#\n",
    "########################################################################\n",
    "#\n",
    "# This file is part of the TensorFlow Tutorials available at:\n",
    "#\n",
    "# https://github.com/Hvass-Labs/TensorFlow-Tutorials\n",
    "#\n",
    "# Published under the MIT License. See the file LICENSE for details.\n",
    "#\n",
    "# Copyright 2016 by Magnus Erik Hvass Pedersen\n",
    "#\n",
    "########################################################################\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import zipfile\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def _print_download_progress(count, block_size, total_size):\n",
    "    \"\"\"\n",
    "    Function used for printing the download progress.\n",
    "    Used as a call-back function in maybe_download_and_extract().\n",
    "    \"\"\"\n",
    "\n",
    "    # Percentage completion.\n",
    "    pct_complete = float(count * block_size) / total_size\n",
    "\n",
    "    # Status-message. Note the \\r which means the line should overwrite itself.\n",
    "    msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n",
    "\n",
    "    # Print it.\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def maybe_download_and_extract(url, download_dir):\n",
    "    \"\"\"\n",
    "    Download and extract the data if it doesn't already exist.\n",
    "    Assumes the url is a tar-ball file.\n",
    "    :param url:\n",
    "        Internet URL for the tar-file to download.\n",
    "        Example: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    :param download_dir:\n",
    "        Directory where the downloaded file is saved.\n",
    "        Example: \"data/CIFAR-10/\"\n",
    "    :return:\n",
    "        Nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filename for saving the file downloaded from the internet.\n",
    "    # Use the filename from the URL and add it to the download_dir.\n",
    "    filename = url.split('/')[-1]\n",
    "    file_path = os.path.join(download_dir, filename)\n",
    "\n",
    "    # Check if the file already exists.\n",
    "    # If it exists then we assume it has also been extracted,\n",
    "    # otherwise we need to download and extract it now.\n",
    "    if not os.path.exists(file_path):\n",
    "        # Check if the download directory exists, otherwise create it.\n",
    "        if not os.path.exists(download_dir):\n",
    "            os.makedirs(download_dir)\n",
    "\n",
    "        # Download the file from the internet.\n",
    "        file_path, _ = urllib.request.urlretrieve(url=url,\n",
    "                                                  filename=file_path,\n",
    "                                                  reporthook=_print_download_progress)\n",
    "\n",
    "        print()\n",
    "        print(\"Download finished. Extracting files.\")\n",
    "\n",
    "        if file_path.endswith(\".zip\"):\n",
    "            # Unpack the zip-file.\n",
    "            zipfile.ZipFile(file=file_path, mode=\"r\").extractall(download_dir)\n",
    "        elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
    "            # Unpack the tar-ball.\n",
    "            tarfile.open(name=file_path, mode=\"r:gz\").extractall(download_dir)\n",
    "\n",
    "        print(\"Done.\")\n",
    "    else:\n",
    "        print(\"Data has apparently already been downloaded and unpacked.\")\n",
    "\n",
    "\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData(url, data_dir):\n",
    "    \"\"\"\n",
    "    Download and extract the data if it doesn't already exist.\n",
    "    Assumes the url is a tar-ball file.\n",
    "    :param url:\n",
    "        Internet URL for the tar-file to download.\n",
    "        Example: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    :param data_dir:\n",
    "        Directory where the downloaded file is saved.\n",
    "        Example: \"data/CIFAR-10/\"\n",
    "    :return:\n",
    "        dict\n",
    "    \"\"\"\n",
    "\n",
    "    # use function maybe_download_and_extract from download.py to download CIFAR-10 data\n",
    "    maybe_download_and_extract(url, data_dir)\n",
    "\n",
    "    # each data file unpickled from CIFAR-10 has dict keys:  dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
    "    for i in range(4):\n",
    "        # Load the images and class-numbers from the data-file.\n",
    "        dict = unpickle(data_dir + \"cifar-10-batches-py/data_batch_\" + str(i + 1))\n",
    "\t\t\t\t# print(\"\\ndict keys: \", dict.keys())\n",
    "        if(i == 0):\n",
    "            X = dict[b'data']\n",
    "            Y = dict[b'labels']\n",
    "        else:\n",
    "            X = np.concatenate((X, dict[b'data'])) \n",
    "            Y = np.concatenate((Y, dict[b'labels']))\n",
    "\n",
    "    X_train = np.concatenate((X, dict[b'data'])).T \n",
    "    Y_train = np.concatenate((Y, dict[b'labels']))\n",
    "\n",
    "    dict = unpickle(data_dir + \"cifar-10-batches-py/test_batch\");\n",
    "    X_test = dict[b'data'].T\n",
    "    Y_test = np.array(dict[b'labels'])\n",
    "    Y_train = np.reshape(Y_train, (1, len(Y_train)))\n",
    "    Y_test = np.reshape(Y_test, (1, len(Y_test)))\n",
    "\n",
    "    raw = unpickle(data_dir + \"cifar-10-batches-py/batches.meta\")[b'label_names']\n",
    "    # Convert from binary strings.\n",
    "    names = [x.decode('utf-8') for x in raw]\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test, names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/media/duo/extra'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-95541dc153bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0morigtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/media/duo/extra/python/tests/data/CIFAR-10/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#reduce examples and tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-e9bb14b32bdb>\u001b[0m in \u001b[0;36mloadData\u001b[0;34m(url, data_dir)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# use function maybe_download_and_extract from download.py to download CIFAR-10 data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmaybe_download_and_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# each data file unpickled from CIFAR-10 has dict keys:  dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-de98b8dad7ff>\u001b[0m in \u001b[0;36mmaybe_download_and_extract\u001b[0;34m(url, download_dir)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Check if the download directory exists, otherwise create it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# Download the file from the internet.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/duo/anaconda3/lib/python3.6/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/duo/anaconda3/lib/python3.6/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/duo/anaconda3/lib/python3.6/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/duo/anaconda3/lib/python3.6/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtail\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;31m# Defeats race condition when another thread created the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/duo/anaconda3/lib/python3.6/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/media/duo/extra'"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "origtrain_x, train_y, origtest_x, test_y, classes = loadData(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", \"/media/duo/extra/python/tests/data/CIFAR-10/\")\n",
    "\n",
    "#reduce examples and tests\n",
    "origtrain_x = origtrain_x[0:,0:400]\n",
    "train_y = train_y[0:,0:400]\n",
    "origtest_x = origtest_x[0:,0:50]\n",
    "test_y = test_y[0:,0:50]\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1\n",
    "train_x = origtrain_x/255\n",
    "test_x = origtest_x/255\n",
    "\n",
    "# Explore your dataset\n",
    "m_train = train_x.shape[1]\n",
    "num_px = train_x.shape[0]\n",
    "m_test = test_x.shape[1]\n",
    "\n",
    "print(\"Number of training examples: \" + str(m_train))\n",
    "print(\"Number of testing examples: \" + str(m_test))\n",
    "print(\"train_x shape: \" + str(train_x.shape))\n",
    "print(\"train_y shape: \" + str(train_y.shape))\n",
    "print(\"test_x shape: \" + str(test_x.shape))\n",
    "print(\"test_y shape: \" + str(test_y.shape))\n",
    "print(\"classes: \", classes)\n",
    "\n",
    "# build and test a 2 layer neural network\n",
    "### CONSTANTS DEFINING THE MODEL ####\n",
    "n_x = 3072\n",
    "# num_px * num_px * 3\n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)\n",
    "\n",
    "\n",
    "parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
